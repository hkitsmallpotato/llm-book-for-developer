# Ch 1 Attention and Transformer

https://www.3blue1brown.com/topics/neural-networks


https://martinlwx.github.io/en/the-bpe-tokenizer/

https://cohere.com/llmu/sentence-word-embeddings

https://datajenius.com/2022/03/13/a-deep-dive-into-nlp-tokenization-encoding-word-embeddings-sentence-embeddings-word2vec-bert/

https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html

https://www.lesswrong.com/posts/tM84DyBg4Jbq5zGmH/linda-linsefors-s-shortform?commentId=owWTRrnGDfEqyGzjb
https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85


https://blog.eleuther.ai/rotary-embeddings/

https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html

https://nn.labml.ai/transformers/rope/index.html
https://nn.labml.ai/transformers/gpt/index.html



https://jalammar.github.io/illustrated-transformer/
http://jalammar.github.io/illustrated-gpt2/





# Ch 2 Training LLM and Text Generation

Chinhilla scaling law paper

https://huggingface.co/blog/how-to-generate




https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web
https://simonwillison.net/2023/Apr/2/calculator-for-words/



https://thegradient.pub/othello/

https://ericjmichaud.com/grokking-squared/

https://imtiazhumayun.github.io/grokking/

https://arxiv.org/abs/2201.02177
https://arxiv.org/abs/2205.10343
https://arxiv.org/abs/2309.02390

https://arxiv.org/abs/2408.04666

https://mlu-explain.github.io/double-descent/
https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/

https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/
https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers



# Ch 3 Emergence, In Context Learning, and Prompt Engineering

https://thegradient.pub/in-context-learning-in-context/

https://ai.stanford.edu/blog/understanding-incontext/

https://www.lakera.ai/blog/what-is-in-context-learning

https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/

https://www.jasonwei.net/blog/emergence

https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/


https://www.promptingguide.ai/

https://learnprompting.org/docs/basics/prompt_engineering

https://learnprompting.org/docs/basics/formalizing



# Ch 4 Running Open source LLM






https://a16z.com/emerging-architectures-for-llm-applications/

https://arxiv.org/abs/2307.09793

https://llmmodels.org/

https://llm.extractum.io/






https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/

https://www.redditmedia.com/r/LocalLLaMA/comments/1atghbb/local_llm_glossary_simple_llama_sillytavern_setup/

https://interconnected.org/home/2024/07/19/ai-landscape



https://hackernoon.com/efficient-guided-generation-for-large-language-models-llm-sampling-and-guided-generation
https://arxiv.org/abs/2307.09702

https://simmering.dev/blog/structured_output/

https://github.com/sgl-project/sglang

https://uptodata.substack.com/p/guided-generation-for-llm-outputs



https://thenewstack.io/a-comprehensive-guide-to-function-calling-in-llms/

https://thenewstack.io/building-an-open-llm-app-using-hermes-2-pro-deployed-locally/

https://ai.google.dev/gemini-api/docs/function-calling

https://e2b.dev/blog/how-to-add-code-interpreter-to-llama3
https://dev.to/tereza_tizkova/llama-3-with-function-calling-and-code-interpreter-55nb


----

# Others

https://nuvalence.io/insights/a-6-category-taxonomy-for-generative-ai-use-cases/

https://towardsai.net/p/artificial-intelligence/generative-ai-terminology-an-evolving-taxonomy-to-get-you-started


