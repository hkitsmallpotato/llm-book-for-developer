# Ch 1 Attention and Transformer

https://www.3blue1brown.com/topics/neural-networks


https://martinlwx.github.io/en/the-bpe-tokenizer/

https://cohere.com/llmu/sentence-word-embeddings

https://datajenius.com/2022/03/13/a-deep-dive-into-nlp-tokenization-encoding-word-embeddings-sentence-embeddings-word2vec-bert/

https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html

https://www.lesswrong.com/posts/tM84DyBg4Jbq5zGmH/linda-linsefors-s-shortform?commentId=owWTRrnGDfEqyGzjb
https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85


https://blog.eleuther.ai/rotary-embeddings/

https://cyrilzakka.github.io/llm-playbook/nested/rot-pos-embed.html

https://nn.labml.ai/transformers/rope/index.html
https://nn.labml.ai/transformers/gpt/index.html



https://jalammar.github.io/illustrated-transformer/
http://jalammar.github.io/illustrated-gpt2/





# Ch 2 Training LLM and Text Generation

Chinhilla scaling law paper

https://huggingface.co/blog/how-to-generate




https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web
https://simonwillison.net/2023/Apr/2/calculator-for-words/



https://thegradient.pub/othello/

https://ericjmichaud.com/grokking-squared/

https://imtiazhumayun.github.io/grokking/

https://arxiv.org/abs/2201.02177
https://arxiv.org/abs/2205.10343
https://arxiv.org/abs/2309.02390

https://arxiv.org/abs/2408.04666

https://mlu-explain.github.io/double-descent/
https://iclr-blogposts.github.io/2024/blog/double-descent-demystified/

https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/
https://www.lesswrong.com/posts/43C3igfmMrE9Qoyfe/scaffolded-llms-as-natural-language-computers



# Ch 3 Emergence, In Context Learning, and Prompt Engineering

https://thegradient.pub/in-context-learning-in-context/

https://ai.stanford.edu/blog/understanding-incontext/

https://www.lakera.ai/blog/what-is-in-context-learning

https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/

https://www.jasonwei.net/blog/emergence

https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/


https://www.promptingguide.ai/

https://learnprompting.org/docs/basics/prompt_engineering

https://learnprompting.org/docs/basics/formalizing



# Ch 4 Running Open source LLM






https://a16z.com/emerging-architectures-for-llm-applications/

https://arxiv.org/abs/2307.09793

https://llmmodels.org/

https://llm.extractum.io/






https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/

https://www.redditmedia.com/r/LocalLLaMA/comments/1atghbb/local_llm_glossary_simple_llama_sillytavern_setup/

https://interconnected.org/home/2024/07/19/ai-landscape



https://hackernoon.com/efficient-guided-generation-for-large-language-models-llm-sampling-and-guided-generation
https://arxiv.org/abs/2307.09702

https://simmering.dev/blog/structured_output/

https://github.com/sgl-project/sglang

https://uptodata.substack.com/p/guided-generation-for-llm-outputs



https://thenewstack.io/a-comprehensive-guide-to-function-calling-in-llms/

https://thenewstack.io/building-an-open-llm-app-using-hermes-2-pro-deployed-locally/

https://ai.google.dev/gemini-api/docs/function-calling

https://e2b.dev/blog/how-to-add-code-interpreter-to-llama3
https://dev.to/tereza_tizkova/llama-3-with-function-calling-and-code-interpreter-55nb


----

# Others

https://nuvalence.io/insights/a-6-category-taxonomy-for-generative-ai-use-cases/

https://towardsai.net/p/artificial-intelligence/generative-ai-terminology-an-evolving-taxonomy-to-get-you-started

----

# Second round

https://thegabmeister.com/blog/run-open-llm-local/

https://vercel.com/guides/openai-function-calling


https://sumanthrh.com/post/distributed-and-efficient-finetuning/

https://kaitchup.substack.com/p/a-guide-on-hyperparameters-and-training

https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft

https://docs.adapterhub.ml/methods.html


https://graphcore-research.github.io/galore/
https://medium.com/@tanalpha-aditya/galore-memory-efficient-llm-training-by-gradient-low-rank-projection-d93390e110fe


https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/


https://insujang.github.io/2024-01-07/llm-inference-autoregressive-generation-and-attention-kv-cache/
https://insujang.github.io/2024-01-11/tensor-parallelism-and-sequence-parallelism-detailed-analysis/
https://insujang.github.io/2024-01-21/flash-attention/


https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8


https://medium.com/ai-science/speculative-decoding-make-llm-inference-faster-c004501af120

https://predibase.com/blog/lorax-the-open-source-framework-for-serving-100s-of-fine-tuned-llms-in
https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one


https://alexgarcia.xyz/blog/2024/sqlite-vec-stable-release/index.html




https://kai-greshake.de/posts/llm-malware/

https://kai-greshake.de/posts/in-escalating-order-of-stupidity/


https://medium.com/emalpha/safeguarding-llm-conversations-using-llama-guard-a1652da1d2de

https://blog.langchain.dev/rebuff/
https://www.guardrailsai.com/docs/concepts/guard



https://lmsys.org/blog/2024-02-05-compressed-fsm/

https://blog.dottxt.co/how-fast-cfg.html

https://lmsys.org/blog/2024-01-17-sglang/


https://pub.towardsai.net/autonomous-gpt-4-from-chatgpt-to-autogpt-agentgpt-babyagi-hugginggpt-and-beyond-9871ceabd69e
https://sea.mashable.com/tech/23298/auto-gpt-babyagi-and-agentgpt-how-to-use-ai-agents


https://lilianweng.github.io/posts/2023-06-23-agent/
https://medium.com/the-modern-scientist/a-complete-guide-to-llms-based-autonomous-agents-part-i-69515c016792


https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/

https://www.width.ai/post/llm-powered-autonomous-agents

https://blog.langchain.dev/what-is-a-cognitive-architecture/

https://github.com/ysymyth/awesome-language-agents


https://sajalsharma.com/posts/overview-multi-agent-fameworks/
https://wandb.ai/vincenttu/blog_posts/reports/Exploring-2-Multi-Agent-LLM-Libraries-Camel-Langroid--Vmlldzo1MzAyODM5


https://arxiv.org/abs/2402.01680
https://arxiv.org/abs/2304.03442

https://arxiv.org/abs/2305.16291


https://docs.ray.io/en/latest/serve/tutorials/vllm-example.html
http://kubeagi.k8s.com.cn/docs/Configuration/DistributedInference/deploy-using-rary-serve/




